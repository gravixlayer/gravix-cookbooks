{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "title"
      },
      "source": [
        "Advanced RAG: Contextual RAG\n",
        "========================================\n",
        "\n",
        "Welcome to the Gravix Layer Advanced Cookbook! This notebook offers a comprehensive, hands-on guide to building a sophisticated¬†**Contextual Retrieval-Augmented Generation (RAG)**¬†system using Gravix Layer's powerful Inference APIs and Vector Database Service. Designed for seamless integration of conversational context and dynamic retrieval, this system elevates traditional RAG by incorporating advanced contextual understanding and query enhancement.\n",
        "\n",
        "What You'll Learn:\n",
        "------------------\n",
        "-   How to configure Gravix Layer APIs for semantic search and context-aware text generation\n",
        "-   How to ingest and process diverse document formats to build a robust knowledge base\n",
        "-   How to implement contextual retrieval with query expansion and reranking for precise results\n",
        "-   How to leverage conversation history for personalized, coherent responses\n",
        "-   How to manage and extend your RAG pipeline with advanced features like metadata filtering and conversation export\n",
        "\n",
        "Who Is This For?\n",
        "----------------\n",
        "\n",
        "-   AI developers and data scientists aiming to build cutting-edge RAG systems with contextual intelligence\n",
        "-   Researchers and engineers exploring Gravix Layer's SDK for interactive, real-world applications\n",
        "-   Enthusiasts interested in conversational AI and advanced retrieval techniques\n",
        "\n",
        "Architecture Overview\n",
        "---------------------\n",
        "\n",
        "1.  **Document Ingestion & Chunking**: Process multi-format documents (PDF, DOCX, TXT, Markdown) into contextually aware chunks with rich metadata\n",
        "2.  **Vector Storage & Embedding**: Convert text into embeddings using Gravix Layer's vector API and store them in a scalable vector database\n",
        "3.  **Contextual Retrieval**: Perform semantic search with dynamic query expansion and reranking, leveraging conversation history for relevance\n",
        "4.  **Context-Aware Generation**: Combine retrieved context and conversational memory with Gravix Layer's LLM for accurate, coherent responses\n",
        "5.  **Memory & Interaction Management**: Maintain persistent conversation history and support real-time document uploads and interactive querying\n",
        "\n",
        "This implementation goes beyond traditional RAG by integrating conversation-aware retrieval, multi-stage search, and personalized response generation, making it ideal for interactive applications like chatbots, knowledge assistants, and research tools. Let's dive in and unlock the full potential of Contextual RAG with Gravix Layer!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "install_packages"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ All packages installed successfully!\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "!pip install gravixlayer PyPDF2 python-docx requests python-dotenv -q\n",
        "print(\"‚úÖ All packages installed successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import tempfile\n",
        "import time\n",
        "import json\n",
        "import re\n",
        "import uuid\n",
        "from typing import List, Optional, Tuple, Any, Dict\n",
        "from datetime import datetime\n",
        "from collections import defaultdict, deque\n",
        "\n",
        "import requests\n",
        "import PyPDF2\n",
        "import docx\n",
        "from gravixlayer import GravixLayer\n",
        "from dotenv import load_dotenv\n",
        "from IPython.display import display, Markdown, HTML"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "imports"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìã Contextual RAG Configuration:\n",
            "  base_url: https://api.gravixlayer.com/v1\n",
            "  embedding_model: baai/bge-large-en-v1.5\n",
            "  llm_model: meta-llama/llama-3.1-8b-instruct\n",
            "  vector_dimension: 1024\n",
            "  similarity_metric: cosine\n",
            "  chunk_size: 800\n",
            "  chunk_overlap: 150\n",
            "  max_conversation_history: 10\n",
            "  retrieval_k: 5\n",
            "  rerank_k: 3\n",
            "\n",
            "‚úÖ Imports and configuration complete!\n"
          ]
        }
      ],
      "source": [
        "# Load environment variables\n",
        "load_dotenv()\n",
        "\n",
        "# Configuration\n",
        "GRAVIX_API_KEY = os.getenv('GRAVIXLAYER_API_KEY')\n",
        "if not GRAVIX_API_KEY:\n",
        "    print(\"‚ö†Ô∏è  GRAVIXLAYER_API_KEY not found in environment variables\")\n",
        "    print(\"Please set it using: set_api_key('your_api_key_here')\")\n",
        "\n",
        "# Global configuration\n",
        "CONFIG = {\n",
        "    'base_url': 'https://api.gravixlayer.com/v1',\n",
        "    'embedding_model': 'baai/bge-large-en-v1.5',\n",
        "    'llm_model': 'meta-llama/llama-3.1-8b-instruct',\n",
        "    'vector_dimension': 1024,\n",
        "    'similarity_metric': 'cosine',\n",
        "    'chunk_size': 800,\n",
        "    'chunk_overlap': 150,\n",
        "    'max_conversation_history': 10,\n",
        "    'retrieval_k': 5,\n",
        "    'rerank_k': 3\n",
        "}\n",
        "\n",
        "print(\"üìã Contextual RAG Configuration:\")\n",
        "for key, value in CONFIG.items():\n",
        "    print(f\"  {key}: {value}\")\n",
        "print(\"\\n‚úÖ Imports and configuration complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "session_state"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìù Session state initialized\n",
            "üí¨ Conversation history limit: 10 interactions\n"
          ]
        }
      ],
      "source": [
        "# Initialize global session state for notebook\n",
        "def init_session_state() -> None:\n",
        "    \"\"\"Initialize session state variables for the notebook\"\"\"\n",
        "    global session_state\n",
        "    try:\n",
        "        session_state\n",
        "    except NameError:\n",
        "        session_state = {\n",
        "            \"api_key_submitted\": False,\n",
        "            \"gravix_api_key\": GRAVIX_API_KEY or \"\",\n",
        "            \"index_id\": \"\",\n",
        "            \"chat_history\": deque(maxlen=CONFIG['max_conversation_history']),\n",
        "            \"processed_files\": [],\n",
        "            \"document_chunks\": {},\n",
        "            \"conversation_context\": {},\n",
        "            \"last_query\": \"\",\n",
        "            \"last_response\": \"\",\n",
        "            \"retrieval_metadata\": {},\n",
        "            \"total_chunks_stored\": 0\n",
        "        }\n",
        "\n",
        "# Helper functions for API key management\n",
        "def set_api_key(api_key: str) -> None:\n",
        "    \"\"\"Set the GravixLayer API key\"\"\"\n",
        "    init_session_state()\n",
        "    session_state['gravix_api_key'] = api_key\n",
        "    session_state['api_key_submitted'] = True\n",
        "    os.environ['GRAVIXLAYER_API_KEY'] = api_key\n",
        "    print(f\"‚úÖ API key set successfully\")\n",
        "\n",
        "def verify_api_key(api_key: str = None) -> bool:\n",
        "    \"\"\"Verify the GravixLayer API key\"\"\"\n",
        "    init_session_state()\n",
        "    key = api_key or session_state.get('gravix_api_key') or GRAVIX_API_KEY\n",
        "    if not key:\n",
        "        print(\"‚ùå No API key provided\")\n",
        "        return False\n",
        "    \n",
        "    try:\n",
        "        headers = {\"Authorization\": f\"Bearer {key}\"}\n",
        "        response = requests.get(f\"{CONFIG['base_url']}/vectors/indexes\", headers=headers)\n",
        "        if response.status_code == 200:\n",
        "            set_api_key(key)\n",
        "            print(\"‚úÖ API key verified successfully!\")\n",
        "            return True\n",
        "        else:\n",
        "            print(f\"‚ùå API key verification failed: {response.text}\")\n",
        "            return False\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error verifying API key: {e}\")\n",
        "        return False\n",
        "\n",
        "def ensure_client():\n",
        "    \"\"\"Ensure GravixLayer client is properly initialized\"\"\"\n",
        "    init_session_state()\n",
        "    api_key = session_state.get(\"gravix_api_key\") or GRAVIX_API_KEY\n",
        "    if not api_key:\n",
        "        raise ValueError(\"GravixLayer API key not provided. Use set_api_key() or verify_api_key()\")\n",
        "    os.environ['GRAVIXLAYER_API_KEY'] = api_key\n",
        "    return GravixLayer()\n",
        "\n",
        "init_session_state()\n",
        "print(\"üìù Session state initialized\")\n",
        "print(f\"üí¨ Conversation history limit: {CONFIG['max_conversation_history']} interactions\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "vector_operations"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Vector operations functions ready\n"
          ]
        }
      ],
      "source": [
        "def create_vector_index(name: str, dimension: int = None) -> Optional[str]:\n",
        "    \"\"\"Create a new vector index using GravixLayer API\"\"\"\n",
        "    init_session_state()\n",
        "    api_key = session_state.get('gravix_api_key')\n",
        "    if not api_key:\n",
        "        print(\"‚ùå No API key available. Use verify_api_key() first.\")\n",
        "        return None\n",
        "    \n",
        "    dimension = dimension or CONFIG['vector_dimension']\n",
        "    \n",
        "    try:\n",
        "        response = requests.post(\n",
        "            f\"{CONFIG['base_url']}/vectors/indexes\",\n",
        "            headers={\n",
        "                \"Authorization\": f\"Bearer {api_key}\",\n",
        "                \"Content-Type\": \"application/json\",\n",
        "            },\n",
        "            json={\n",
        "                \"name\": name,\n",
        "                \"dimension\": dimension,\n",
        "                \"metric\": CONFIG['similarity_metric']\n",
        "            }\n",
        "        )\n",
        "        \n",
        "        if response.status_code in [200, 201]:\n",
        "            index_data = response.json()\n",
        "            index_id = index_data.get(\"id\")\n",
        "            session_state['index_id'] = index_id\n",
        "            print(f\"‚úÖ Created vector index: {index_id}\")\n",
        "            return index_id\n",
        "        else:\n",
        "            print(f\"‚ùå Failed to create index: {response.text}\")\n",
        "            return None\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error creating vector index: {e}\")\n",
        "        return None\n",
        "\n",
        "def list_vector_indexes() -> List[Dict]:\n",
        "    \"\"\"List all available vector indexes\"\"\"\n",
        "    init_session_state()\n",
        "    api_key = session_state.get('gravix_api_key')\n",
        "    if not api_key:\n",
        "        return []\n",
        "    \n",
        "    try:\n",
        "        response = requests.get(\n",
        "            f\"{CONFIG['base_url']}/vectors/indexes\",\n",
        "            headers={\"Authorization\": f\"Bearer {api_key}\"}\n",
        "        )\n",
        "        if response.status_code == 200:\n",
        "            return response.json().get(\"indexes\", [])\n",
        "        return []\n",
        "    except Exception:\n",
        "        return []\n",
        "\n",
        "def find_or_create_index(name: str) -> Optional[str]:\n",
        "    \"\"\"Find existing index by name or create new one\"\"\"\n",
        "    indexes = list_vector_indexes()\n",
        "    for idx in indexes:\n",
        "        if idx.get('name') == name:\n",
        "            index_id = idx.get('id')\n",
        "            session_state['index_id'] = index_id\n",
        "            print(f\"‚úÖ Using existing index: {index_id}\")\n",
        "            return index_id\n",
        "    \n",
        "    return create_vector_index(name)\n",
        "\n",
        "def upsert_vectors(text_chunks: List[str], metadata_list: List[Dict]) -> bool:\n",
        "    \"\"\"Upload text chunks to GravixLayer vector database\"\"\"\n",
        "    init_session_state()\n",
        "    api_key = session_state.get('gravix_api_key')\n",
        "    index_id = session_state.get('index_id')\n",
        "    \n",
        "    if not api_key or not index_id:\n",
        "        print(\"‚ùå Missing API key or index ID\")\n",
        "        return False\n",
        "    \n",
        "    try:\n",
        "        vectors_data = {\"vectors\": []}\n",
        "        \n",
        "        for i, (chunk, metadata) in enumerate(zip(text_chunks, metadata_list)):\n",
        "            chunk_id = f\"{metadata.get('filename', 'doc')}_{i}_{uuid.uuid4().hex[:8]}\"\n",
        "            \n",
        "            vector_entry = {\n",
        "                \"id\": chunk_id,\n",
        "                \"text\": chunk,\n",
        "                \"model\": CONFIG['embedding_model'],\n",
        "                \"metadata\": {\n",
        "                    **metadata,\n",
        "                    \"chunk_id\": chunk_id,\n",
        "                    \"chunk_index\": i,\n",
        "                    \"timestamp\": datetime.now().isoformat(),\n",
        "                    \"total_chunks\": len(text_chunks)\n",
        "                },\n",
        "                \"delete_protection\": False\n",
        "            }\n",
        "            vectors_data[\"vectors\"].append(vector_entry)\n",
        "            \n",
        "            # Store in session for retrieval\n",
        "            session_state['document_chunks'][chunk_id] = chunk\n",
        "        \n",
        "        response = requests.post(\n",
        "            f\"{CONFIG['base_url']}/vectors/{index_id}/text/upsert\",\n",
        "            headers={\n",
        "                \"Authorization\": f\"Bearer {api_key}\",\n",
        "                \"Content-Type\": \"application/json\"\n",
        "            },\n",
        "            json=vectors_data\n",
        "        )\n",
        "        \n",
        "        if response.status_code in [200, 201]:\n",
        "            result = response.json()\n",
        "            upserted_count = result.get('upserted_count', len(text_chunks))\n",
        "            session_state['total_chunks_stored'] += upserted_count\n",
        "            print(f\"‚úÖ Uploaded {upserted_count} chunks successfully\")\n",
        "            return True\n",
        "        else:\n",
        "            print(f\"‚ùå Failed to upload vectors: {response.text}\")\n",
        "            return False\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error uploading vectors: {e}\")\n",
        "        return False\n",
        "\n",
        "def search_vectors(query: str, top_k: int = None, filters: Dict = None) -> List[Dict]:\n",
        "    \"\"\"Search for similar text chunks in vector database\"\"\"\n",
        "    init_session_state()\n",
        "    api_key = session_state.get('gravix_api_key')\n",
        "    index_id = session_state.get('index_id')\n",
        "    top_k = top_k or CONFIG['retrieval_k']\n",
        "    \n",
        "    if not api_key or not index_id:\n",
        "        print(\"‚ùå Missing API key or index ID\")\n",
        "        return []\n",
        "    \n",
        "    try:\n",
        "        search_data = {\n",
        "            \"query\": query,\n",
        "            \"model\": CONFIG['embedding_model'],\n",
        "            \"top_k\": top_k,\n",
        "            \"include_metadata\": True,\n",
        "            \"include_values\": False\n",
        "        }\n",
        "        \n",
        "        if filters:\n",
        "            search_data[\"filter\"] = filters\n",
        "        \n",
        "        response = requests.post(\n",
        "            f\"{CONFIG['base_url']}/vectors/{index_id}/search/text\",\n",
        "            headers={\n",
        "                \"Authorization\": f\"Bearer {api_key}\",\n",
        "                \"Content-Type\": \"application/json\"\n",
        "            },\n",
        "            json=search_data\n",
        "        )\n",
        "        \n",
        "        if response.status_code == 200:\n",
        "            result = response.json()\n",
        "            hits = result.get(\"hits\", [])\n",
        "            print(f\"üîç Found {len(hits)} relevant chunks\")\n",
        "            return hits\n",
        "        else:\n",
        "            print(f\"‚ùå Search failed: {response.text}\")\n",
        "            return []\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error searching vectors: {e}\")\n",
        "        return []\n",
        "\n",
        "print(\"‚úÖ Vector operations functions ready\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "text_processing"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Text processing functions ready\n"
          ]
        }
      ],
      "source": [
        "def extract_text_from_file(file_content: bytes, filename: str) -> str:\n",
        "    \"\"\"Extract text from various file formats with enhanced error handling\"\"\"\n",
        "    try:\n",
        "        file_ext = os.path.splitext(filename)[1].lower()\n",
        "        \n",
        "        if file_ext == '.pdf':\n",
        "            return _extract_pdf_text(file_content)\n",
        "        elif file_ext == '.docx':\n",
        "            return _extract_docx_text(file_content)\n",
        "        elif file_ext in ['.txt', '.md']:\n",
        "            return file_content.decode('utf-8')\n",
        "        else:\n",
        "            # Try as plain text\n",
        "            return file_content.decode('utf-8', errors='ignore')\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Failed to extract text from {filename}: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "def _extract_pdf_text(file_content: bytes) -> str:\n",
        "    \"\"\"Extract text from PDF bytes\"\"\"\n",
        "    with tempfile.NamedTemporaryFile(delete=False, suffix='.pdf') as tmp:\n",
        "        tmp.write(file_content)\n",
        "        tmp_path = tmp.name\n",
        "    \n",
        "    try:\n",
        "        text = \"\"\n",
        "        with open(tmp_path, 'rb') as file:\n",
        "            pdf_reader = PyPDF2.PdfReader(file)\n",
        "            for page_num, page in enumerate(pdf_reader.pages, 1):\n",
        "                try:\n",
        "                    page_text = page.extract_text()\n",
        "                    if page_text.strip():\n",
        "                        text += f\"\\n\\n[PAGE {page_num}]\\n{page_text}\"\n",
        "                except Exception as e:\n",
        "                    print(f\"‚ö†Ô∏è  Warning: Could not extract page {page_num}: {e}\")\n",
        "                    continue\n",
        "        return text.strip()\n",
        "    finally:\n",
        "        os.unlink(tmp_path)\n",
        "\n",
        "def _extract_docx_text(file_content: bytes) -> str:\n",
        "    \"\"\"Extract text from DOCX bytes\"\"\"\n",
        "    with tempfile.NamedTemporaryFile(delete=False, suffix='.docx') as tmp:\n",
        "        tmp.write(file_content)\n",
        "        tmp_path = tmp.name\n",
        "    \n",
        "    try:\n",
        "        doc = docx.Document(tmp_path)\n",
        "        paragraphs = []\n",
        "        for paragraph in doc.paragraphs:\n",
        "            text = paragraph.text.strip()\n",
        "            if text:\n",
        "                paragraphs.append(text)\n",
        "        return \"\\n\\n\".join(paragraphs)\n",
        "    finally:\n",
        "        os.unlink(tmp_path)\n",
        "\n",
        "def chunk_text_contextually(text: str, filename: str, chunk_size: int = None, overlap: int = None) -> List[Dict]:\n",
        "    \"\"\"Split text into overlapping chunks with contextual metadata\"\"\"\n",
        "    chunk_size = chunk_size or CONFIG['chunk_size']\n",
        "    overlap = overlap or CONFIG['chunk_overlap']\n",
        "    \n",
        "    if len(text) <= chunk_size:\n",
        "        return [{\n",
        "            'text': text,\n",
        "            'metadata': {\n",
        "                'filename': filename,\n",
        "                'chunk_number': 0,\n",
        "                'char_start': 0,\n",
        "                'char_end': len(text),\n",
        "                'document_type': _get_document_type(filename)\n",
        "            }\n",
        "        }] if text.strip() else []\n",
        "    \n",
        "    # Enhanced chunking with sentence awareness\n",
        "    sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
        "    chunks = []\n",
        "    current_chunk = \"\"\n",
        "    current_start = 0\n",
        "    chunk_number = 0\n",
        "    \n",
        "    for sentence in sentences:\n",
        "        sentence = sentence.strip()\n",
        "        if not sentence:\n",
        "            continue\n",
        "        \n",
        "        # Check if adding sentence exceeds chunk size\n",
        "        if len(current_chunk) + len(sentence) + 1 > chunk_size and current_chunk:\n",
        "            if len(current_chunk.strip()) >= 50:  # Minimum chunk size\n",
        "                chunk_end = current_start + len(current_chunk)\n",
        "                chunks.append({\n",
        "                    'text': current_chunk.strip(),\n",
        "                    'metadata': {\n",
        "                        'filename': filename,\n",
        "                        'chunk_number': chunk_number,\n",
        "                        'char_start': current_start,\n",
        "                        'char_end': chunk_end,\n",
        "                        'document_type': _get_document_type(filename),\n",
        "                        'has_previous': chunk_number > 0,\n",
        "                        'context_summary': _generate_context_summary(current_chunk)\n",
        "                    }\n",
        "                })\n",
        "                chunk_number += 1\n",
        "            \n",
        "            # Handle overlap for context continuity\n",
        "            words = current_chunk.split()\n",
        "            overlap_words = words[-overlap//10:] if len(words) > overlap//10 else []\n",
        "            overlap_text = \" \".join(overlap_words)\n",
        "            \n",
        "            current_chunk = overlap_text + \" \" + sentence if overlap_words else sentence\n",
        "            current_start = chunk_end - len(overlap_text) if overlap_words else chunk_end\n",
        "        else:\n",
        "            current_chunk += (\" \" + sentence) if current_chunk else sentence\n",
        "    \n",
        "    # Add final chunk\n",
        "    if current_chunk.strip() and len(current_chunk.strip()) >= 50:\n",
        "        chunks.append({\n",
        "            'text': current_chunk.strip(),\n",
        "            'metadata': {\n",
        "                'filename': filename,\n",
        "                'chunk_number': chunk_number,\n",
        "                'char_start': current_start,\n",
        "                'char_end': current_start + len(current_chunk),\n",
        "                'document_type': _get_document_type(filename),\n",
        "                'has_previous': chunk_number > 0,\n",
        "                'is_final': True,\n",
        "                'context_summary': _generate_context_summary(current_chunk)\n",
        "            }\n",
        "        })\n",
        "    \n",
        "    return chunks\n",
        "\n",
        "def _get_document_type(filename: str) -> str:\n",
        "    \"\"\"Determine document type from filename\"\"\"\n",
        "    ext = os.path.splitext(filename)[1].lower()\n",
        "    type_map = {\n",
        "        '.pdf': 'pdf',\n",
        "        '.docx': 'docx', \n",
        "        '.txt': 'text',\n",
        "        '.md': 'markdown'\n",
        "    }\n",
        "    return type_map.get(ext, 'unknown')\n",
        "\n",
        "def _generate_context_summary(text: str) -> str:\n",
        "    \"\"\"Generate a brief context summary for the chunk\"\"\"\n",
        "    # Simple keyword extraction\n",
        "    words = re.findall(r'\\b\\w{4,}\\b', text.lower())\n",
        "    word_freq = {}\n",
        "    for word in words:\n",
        "        word_freq[word] = word_freq.get(word, 0) + 1\n",
        "    \n",
        "    # Get top 5 keywords\n",
        "    top_words = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)[:5]\n",
        "    return \", \".join([word for word, _ in top_words])\n",
        "\n",
        "print(\"‚úÖ Text processing functions ready\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "conversation_management"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Conversation management functions ready\n"
          ]
        }
      ],
      "source": [
        "def add_to_conversation(role: str, content: str, metadata: Dict = None) -> None:\n",
        "    \"\"\"Add a message to the conversation history\"\"\"\n",
        "    init_session_state()\n",
        "    \n",
        "    message = {\n",
        "        \"role\": role,\n",
        "        \"content\": content,\n",
        "        \"timestamp\": datetime.now().isoformat(),\n",
        "        \"metadata\": metadata or {}\n",
        "    }\n",
        "    \n",
        "    session_state['chat_history'].append(message)\n",
        "    \n",
        "    if role == \"user\":\n",
        "        session_state['last_query'] = content\n",
        "    elif role == \"assistant\":\n",
        "        session_state['last_response'] = content\n",
        "\n",
        "def get_conversation_context(include_last_n: int = 3) -> str:\n",
        "    \"\"\"Get recent conversation context for contextual processing\"\"\"\n",
        "    init_session_state()\n",
        "    \n",
        "    if not session_state['chat_history']:\n",
        "        return \"\"\n",
        "    \n",
        "    recent_messages = list(session_state['chat_history'])[-include_last_n * 2:]  # Get Q&A pairs\n",
        "    context_parts = []\n",
        "    \n",
        "    for msg in recent_messages:\n",
        "        role_prefix = \"Human\" if msg['role'] == 'user' else \"Assistant\"\n",
        "        content = msg['content'][:200] + \"...\" if len(msg['content']) > 200 else msg['content']\n",
        "        context_parts.append(f\"{role_prefix}: {content}\")\n",
        "    \n",
        "    return \"\\n\".join(context_parts)\n",
        "\n",
        "def extract_conversation_keywords() -> List[str]:\n",
        "    \"\"\"Extract key terms from recent conversation for context\"\"\"\n",
        "    init_session_state()\n",
        "    \n",
        "    if not session_state['chat_history']:\n",
        "        return []\n",
        "    \n",
        "    # Get text from last 3 interactions\n",
        "    recent_text = \" \".join([\n",
        "        msg['content'] for msg in list(session_state['chat_history'])[-6:]\n",
        "    ])\n",
        "    \n",
        "    # Simple keyword extraction\n",
        "    words = re.findall(r'\\b\\w{4,}\\b', recent_text.lower())\n",
        "    word_freq = {}\n",
        "    for word in words:\n",
        "        if word not in ['this', 'that', 'with', 'have', 'will', 'they', 'from', 'been', 'said']:\n",
        "            word_freq[word] = word_freq.get(word, 0) + 1\n",
        "    \n",
        "    # Return top keywords\n",
        "    top_keywords = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)[:8]\n",
        "    return [word for word, count in top_keywords if count > 1]\n",
        "\n",
        "def clear_conversation() -> None:\n",
        "    \"\"\"Clear conversation history\"\"\"\n",
        "    init_session_state()\n",
        "    session_state['chat_history'].clear()\n",
        "    session_state['conversation_context'].clear()\n",
        "    session_state['last_query'] = \"\"\n",
        "    session_state['last_response'] = \"\"\n",
        "    print(\"üí≠ Conversation history cleared\")\n",
        "\n",
        "def show_conversation_history() -> None:\n",
        "    \"\"\"Display the conversation history\"\"\"\n",
        "    init_session_state()\n",
        "    \n",
        "    if not session_state['chat_history']:\n",
        "        print(\"üìù No conversation history yet\")\n",
        "        return\n",
        "    \n",
        "    display(Markdown(\"### üí¨ Conversation History\"))\n",
        "    \n",
        "    for i, msg in enumerate(session_state['chat_history'], 1):\n",
        "        role_emoji = \"üßë\" if msg['role'] == 'user' else \"ü§ñ\"\n",
        "        role_name = \"You\" if msg['role'] == 'user' else \"Assistant\"\n",
        "        timestamp = datetime.fromisoformat(msg['timestamp']).strftime(\"%H:%M:%S\")\n",
        "        \n",
        "        print(f\"\\n{role_emoji} **{role_name}** ({timestamp}):\")\n",
        "        print(f\"{msg['content']}\")\n",
        "        print(\"‚îÄ\" * 60)\n",
        "\n",
        "print(\"‚úÖ Conversation management functions ready\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "contextual_retrieval"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Contextual retrieval functions ready\n"
          ]
        }
      ],
      "source": [
        "def contextual_search_and_rerank(query: str, conversation_context: str = \"\", \n",
        "                                top_k: int = None) -> Tuple[List[Dict], str]:\n",
        "    \"\"\"Perform contextual search with query expansion and reranking\"\"\"\n",
        "    top_k = top_k or CONFIG['retrieval_k']\n",
        "    \n",
        "    print(f\"üîç Starting contextual search for: '{query[:50]}...'\")\n",
        "    \n",
        "    # Step 1: Generate expanded queries\n",
        "    expanded_queries = _expand_query_contextually(query, conversation_context)\n",
        "    print(f\"üìù Generated {len(expanded_queries)} search variations\")\n",
        "    \n",
        "    # Step 2: Search with multiple queries\n",
        "    all_hits = []\n",
        "    for expanded_query in expanded_queries:\n",
        "        hits = search_vectors(expanded_query, top_k=top_k * 2)  # Get more for reranking\n",
        "        all_hits.extend(hits)\n",
        "    \n",
        "    # Step 3: Deduplicate and merge scores\n",
        "    unique_hits = _deduplicate_hits(all_hits)\n",
        "    print(f\"üéØ Found {len(unique_hits)} unique results\")\n",
        "    \n",
        "    # Step 4: Contextual reranking\n",
        "    reranked_hits = _rerank_by_context(unique_hits, query, conversation_context)\n",
        "    final_hits = reranked_hits[:CONFIG['rerank_k']]\n",
        "    \n",
        "    print(f\"‚≠ê Selected top {len(final_hits)} most relevant chunks\")\n",
        "    \n",
        "    # Step 5: Build context string\n",
        "    context_string = _build_context_string(final_hits, query, conversation_context)\n",
        "    \n",
        "    return final_hits, context_string\n",
        "\n",
        "def _expand_query_contextually(query: str, conversation_context: str) -> List[str]:\n",
        "    \"\"\"Expand query based on conversation context\"\"\"\n",
        "    expanded_queries = [query]  # Original query\n",
        "    \n",
        "    # Add conversation keywords if available\n",
        "    conversation_keywords = extract_conversation_keywords()\n",
        "    if conversation_keywords:\n",
        "        # Create keyword-enhanced query\n",
        "        keyword_query = f\"{query} {' '.join(conversation_keywords[:3])}\"\n",
        "        expanded_queries.append(keyword_query)\n",
        "    \n",
        "    # Create semantic variations\n",
        "    query_words = query.lower().split()\n",
        "    if len(query_words) > 2:\n",
        "        # Partial query with key terms\n",
        "        partial_query = ' '.join(query_words[:2] + query_words[-1:])\n",
        "        expanded_queries.append(partial_query)\n",
        "    \n",
        "    # Add context-specific variations based on question type\n",
        "    if query.lower().startswith(('what', 'how', 'why', 'when', 'where')):\n",
        "        # Question-specific expansion\n",
        "        base_terms = ' '.join(query_words[1:])  # Remove question word\n",
        "        expanded_queries.append(base_terms)\n",
        "    \n",
        "    return list(set(expanded_queries))  # Remove duplicates\n",
        "\n",
        "def _deduplicate_hits(hits: List[Dict]) -> List[Dict]:\n",
        "    \"\"\"Remove duplicate hits and merge scores\"\"\"\n",
        "    hit_dict = {}\n",
        "    \n",
        "    for hit in hits:\n",
        "        hit_id = hit.get('id')\n",
        "        if hit_id in hit_dict:\n",
        "            # Keep hit with higher score\n",
        "            existing_score = hit_dict[hit_id].get('score', 0)\n",
        "            new_score = hit.get('score', 0)\n",
        "            if new_score > existing_score:\n",
        "                hit_dict[hit_id] = hit\n",
        "        else:\n",
        "            hit_dict[hit_id] = hit\n",
        "    \n",
        "    # Sort by score descending\n",
        "    return sorted(hit_dict.values(), key=lambda x: x.get('score', 0), reverse=True)\n",
        "\n",
        "def _rerank_by_context(hits: List[Dict], query: str, conversation_context: str) -> List[Dict]:\n",
        "    \"\"\"Rerank hits based on contextual relevance\"\"\"\n",
        "    conversation_keywords = extract_conversation_keywords()\n",
        "    \n",
        "    for hit in hits:\n",
        "        base_score = hit.get('score', 0)\n",
        "        context_bonus = 0\n",
        "        \n",
        "        # Get chunk text for analysis\n",
        "        chunk_id = hit.get('id')\n",
        "        chunk_text = session_state['document_chunks'].get(chunk_id, '').lower()\n",
        "        \n",
        "        if chunk_text:\n",
        "            # Bonus for conversation continuity\n",
        "            if conversation_keywords:\n",
        "                keyword_matches = sum(1 for kw in conversation_keywords if kw in chunk_text)\n",
        "                context_bonus += keyword_matches * 0.05\n",
        "            \n",
        "            # Bonus for query type relevance\n",
        "            if query.lower().startswith(('what', 'define', 'explain')):\n",
        "                if any(term in chunk_text for term in ['definition', 'means', 'refers to']):\n",
        "                    context_bonus += 0.1\n",
        "            elif query.lower().startswith(('how', 'process', 'method')):\n",
        "                if any(term in chunk_text for term in ['steps', 'process', 'method', 'approach']):\n",
        "                    context_bonus += 0.1\n",
        "            elif query.lower().startswith(('why', 'reason', 'cause')):\n",
        "                if any(term in chunk_text for term in ['because', 'reason', 'cause', 'due to']):\n",
        "                    context_bonus += 0.1\n",
        "            \n",
        "            # Bonus for document structure\n",
        "            metadata = hit.get('metadata', {})\n",
        "            if metadata.get('chunk_number', 0) == 0:  # First chunk often contains key info\n",
        "                context_bonus += 0.03\n",
        "        \n",
        "        hit['contextual_score'] = base_score + context_bonus\n",
        "    \n",
        "    return sorted(hits, key=lambda x: x.get('contextual_score', x.get('score', 0)), reverse=True)\n",
        "\n",
        "def _build_context_string(hits: List[Dict], query: str, conversation_context: str) -> str:\n",
        "    \"\"\"Build comprehensive context string for LLM\"\"\"\n",
        "    context_parts = []\n",
        "    \n",
        "    # Add conversation context if relevant\n",
        "    if conversation_context and len(conversation_context) > 50:\n",
        "        context_parts.append(f\"Recent Conversation Context:\\n{conversation_context}\\n\")\n",
        "    \n",
        "    # Add retrieved documents\n",
        "    for i, hit in enumerate(hits, 1):\n",
        "        chunk_id = hit.get('id')\n",
        "        chunk_text = session_state['document_chunks'].get(chunk_id, '')\n",
        "        \n",
        "        if chunk_text:\n",
        "            metadata = hit.get('metadata', {})\n",
        "            filename = metadata.get('filename', 'Unknown')\n",
        "            chunk_num = metadata.get('chunk_index', 0)\n",
        "            doc_type = metadata.get('document_type', 'unknown')\n",
        "            score = hit.get('contextual_score', hit.get('score', 0))\n",
        "            \n",
        "            context_parts.append(\n",
        "                f\"Source {i} [Score: {score:.3f}] - {filename} (chunk {chunk_num}, {doc_type}):\\n{chunk_text}\"\n",
        "            )\n",
        "    \n",
        "    return \"\\n\\n\".join(context_parts)\n",
        "\n",
        "print(\"‚úÖ Contextual retrieval functions ready\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "llm_generation"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ LLM generation functions ready\n"
          ]
        }
      ],
      "source": [
        "def generate_contextual_response(query: str, context: str, conversation_context: str = \"\") -> str:\n",
        "    \"\"\"Generate contextually-aware response using GravixLayer LLM\"\"\"\n",
        "    print(\"ü§ñ Generating contextual response...\")\n",
        "    \n",
        "    try:\n",
        "        client = ensure_client()\n",
        "        \n",
        "        # Build contextual prompt\n",
        "        prompt = _build_contextual_prompt(query, context, conversation_context)\n",
        "        \n",
        "        response = client.chat.completions.create(\n",
        "            model=CONFIG['llm_model'],\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "            max_tokens=600,\n",
        "            temperature=0.1  # Lower temperature for more focused responses\n",
        "        )\n",
        "        \n",
        "        raw_response = response.choices[0].message.content\n",
        "        processed_response = _post_process_response(raw_response)\n",
        "        \n",
        "        return processed_response\n",
        "        \n",
        "    except Exception as e:\n",
        "        return f\"‚ùå Error generating response: {str(e)}\"\n",
        "\n",
        "def _build_contextual_prompt(query: str, context: str, conversation_context: str) -> str:\n",
        "    \"\"\"Build enhanced prompt with contextual instructions\"\"\"\n",
        "    \n",
        "    system_instructions = \"\"\"You are an intelligent assistant with advanced contextual reasoning capabilities. \n",
        "\n",
        "Your task is to provide comprehensive, accurate, and contextually-aware responses by:\n",
        "\n",
        "1. **Context Integration**: Use both document context AND conversation history\n",
        "2. **Conversational Flow**: Maintain continuity with previous interactions\n",
        "3. **Source Accuracy**: Base answers primarily on provided context\n",
        "4. **Reasoning**: Show logical connections between different pieces of information\n",
        "5. **Completeness**: Provide detailed yet focused answers\n",
        "6. **Limitations**: Clearly state when information is insufficient\"\"\"\n",
        "    \n",
        "    conversation_section = \"\"\n",
        "    if conversation_context and len(conversation_context.strip()) > 20:\n",
        "        conversation_section = f\"\\n\\nPrevious Conversation:\\n{conversation_context}\"\n",
        "    \n",
        "    prompt = f\"\"\"{system_instructions}\n",
        "{conversation_section}\n",
        "\n",
        "Document Context:\n",
        "{context}\n",
        "\n",
        "Current Question: {query}\n",
        "\n",
        "Please provide a comprehensive answer that:\n",
        "- Directly addresses the current question\n",
        "- Integrates relevant information from the documents\n",
        "- Maintains consistency with previous conversation\n",
        "- Shows reasoning when connecting concepts\n",
        "- Acknowledges any limitations or gaps in available information\n",
        "\n",
        "Response:\"\"\"\n",
        "    \n",
        "    return prompt\n",
        "\n",
        "def _post_process_response(response: str) -> str:\n",
        "    \"\"\"Clean up and format the generated response\"\"\"\n",
        "    # Remove empty parentheses\n",
        "    response = re.sub(r\"\\(\\s*\\)\", \"\", response)\n",
        "    \n",
        "    # Convert bullet points\n",
        "    response = response.replace(\"‚Ä¢ \", \"\\n- \")\n",
        "    \n",
        "    # Clean up multiple newlines\n",
        "    response = re.sub(r\"\\n{3,}\", \"\\n\\n\", response)\n",
        "    \n",
        "    # Remove trailing/leading whitespace\n",
        "    response = response.strip()\n",
        "    \n",
        "    return response\n",
        "\n",
        "print(\"‚úÖ LLM generation functions ready\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "main_functions"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Main RAG system functions ready\n"
          ]
        }
      ],
      "source": [
        "# Main RAG System Functions\n",
        "\n",
        "def setup_contextual_rag(index_name: str = \"contextual-rag\") -> bool:\n",
        "    \"\"\"Initialize the contextual RAG system\"\"\"\n",
        "    display(Markdown(\"### üöÄ Setting up Contextual RAG System\"))\n",
        "    \n",
        "    # Verify API key\n",
        "    if not verify_api_key():\n",
        "        print(\"‚ùå Setup failed: API key verification failed\")\n",
        "        return False\n",
        "    \n",
        "    # Create or find vector index\n",
        "    print(f\"üîç Setting up vector index: {index_name}\")\n",
        "    index_id = find_or_create_index(index_name)\n",
        "    \n",
        "    if not index_id:\n",
        "        print(\"‚ùå Setup failed: Could not create vector index\")\n",
        "        return False\n",
        "    \n",
        "    print(f\"‚úÖ Contextual RAG system ready!\")\n",
        "    print(f\"   Index ID: {index_id}\")\n",
        "    print(f\"   Configuration: {CONFIG['chunk_size']} chars/chunk, {CONFIG['retrieval_k']} retrieval\")\n",
        "    return True\n",
        "\n",
        "def ingest_document(file_path: str, custom_name: str = None) -> bool:\n",
        "    \"\"\"Ingest a document into the contextual RAG system\"\"\"\n",
        "    if not os.path.exists(file_path):\n",
        "        print(f\"‚ùå File not found: {file_path}\")\n",
        "        return False\n",
        "    \n",
        "    filename = custom_name or os.path.basename(file_path)\n",
        "    display(Markdown(f\"### üìÑ Ingesting Document: `{filename}`\"))\n",
        "    \n",
        "    try:\n",
        "        # Read file content\n",
        "        with open(file_path, 'rb') as f:\n",
        "            file_content = f.read()\n",
        "        \n",
        "        # Extract text\n",
        "        print(\"üîç Extracting text...\")\n",
        "        text = extract_text_from_file(file_content, filename)\n",
        "        \n",
        "        if not text or len(text.strip()) < 50:\n",
        "            print(\"‚ùå Insufficient text extracted from document\")\n",
        "            return False\n",
        "        \n",
        "        print(f\"‚úÖ Extracted {len(text)} characters\")\n",
        "        \n",
        "        # Create contextual chunks\n",
        "        print(\"üß© Creating contextual chunks...\")\n",
        "        chunks = chunk_text_contextually(text, filename)\n",
        "        \n",
        "        if not chunks:\n",
        "            print(\"‚ùå No valid chunks created\")\n",
        "            return False\n",
        "        \n",
        "        print(f\"‚úÖ Created {len(chunks)} chunks\")\n",
        "        \n",
        "        # Extract text and metadata for vector storage\n",
        "        chunk_texts = [chunk['text'] for chunk in chunks]\n",
        "        chunk_metadata = [chunk['metadata'] for chunk in chunks]\n",
        "        \n",
        "        # Upload to vector database\n",
        "        print(\"üíæ Uploading to vector database...\")\n",
        "        success = upsert_vectors(chunk_texts, chunk_metadata)\n",
        "        \n",
        "        if success:\n",
        "            session_state['processed_files'].append(filename)\n",
        "            print(f\"‚úÖ Document '{filename}' successfully ingested!\")\n",
        "            print(f\"üìä Total documents: {len(session_state['processed_files'])}\")\n",
        "            print(f\"üìä Total chunks stored: {session_state['total_chunks_stored']}\")\n",
        "            return True\n",
        "        else:\n",
        "            print(\"‚ùå Failed to upload to vector database\")\n",
        "            return False\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error ingesting document: {str(e)}\")\n",
        "        return False\n",
        "\n",
        "def query_contextual_rag(question: str, show_context: bool = True, \n",
        "                        show_retrieval_info: bool = True) -> Dict[str, Any]:\n",
        "    \"\"\"Query the contextual RAG system\"\"\"\n",
        "    display(Markdown(f\"### üß† Contextual Query: {question}\"))\n",
        "    print(\"‚ïê\" * 80)\n",
        "    \n",
        "    # Get conversation context\n",
        "    conversation_context = get_conversation_context()\n",
        "    \n",
        "    if conversation_context and show_retrieval_info:\n",
        "        print(\"üí≠ Using conversation history for enhanced context...\")\n",
        "    \n",
        "    # Perform contextual search\n",
        "    print(\"üîç Performing contextual retrieval...\")\n",
        "    hits, context = contextual_search_and_rerank(question, conversation_context)\n",
        "    \n",
        "    if not hits:\n",
        "        print(\"‚ùå No relevant documents found\")\n",
        "        add_to_conversation(\"user\", question)\n",
        "        response = \"I couldn't find any relevant information to answer your question. Please try rephrasing or check if documents have been uploaded.\"\n",
        "        add_to_conversation(\"assistant\", response)\n",
        "        return {\"response\": response, \"context\": \"\", \"hits\": []}\n",
        "    \n",
        "    if show_context:\n",
        "        display(Markdown(\"#### üìÑ Retrieved Context:\"))\n",
        "        print(context[:1000] + \"...\" if len(context) > 1000 else context)\n",
        "        print(\"‚îÄ\" * 80)\n",
        "    \n",
        "    # Generate contextual response\n",
        "    print(\"ü§ñ Generating contextual response...\")\n",
        "    response = generate_contextual_response(question, context, conversation_context)\n",
        "    \n",
        "    # Add to conversation history\n",
        "    add_to_conversation(\"user\", question, {\"retrieved_chunks\": len(hits)})\n",
        "    add_to_conversation(\"assistant\", response, {\n",
        "        \"context_used\": bool(conversation_context),\n",
        "        \"sources_used\": len(hits)\n",
        "    })\n",
        "    \n",
        "    # Display response\n",
        "    display(Markdown(\"### üí° Contextual Response:\"))\n",
        "    print(response)\n",
        "    \n",
        "    if show_retrieval_info:\n",
        "        print(f\"\\nüß† Retrieval Summary:\")\n",
        "        print(f\"  ‚Ä¢ Analyzed {len(hits)} relevant chunks\")\n",
        "        print(f\"  ‚Ä¢ Used conversation history: {'Yes' if conversation_context else 'No'}\")\n",
        "        print(f\"  ‚Ä¢ Sources: {', '.join(set([hit.get('metadata', {}).get('filename', 'Unknown') for hit in hits]))}\")\n",
        "    \n",
        "    print(\"‚ïê\" * 80)\n",
        "    \n",
        "    return {\n",
        "        \"query\": question,\n",
        "        \"response\": response,\n",
        "        \"context\": context,\n",
        "        \"hits\": hits,\n",
        "        \"conversation_context_used\": bool(conversation_context)\n",
        "    }\n",
        "\n",
        "def show_system_status() -> None:\n",
        "    \"\"\"Display current system status\"\"\"\n",
        "    init_session_state()\n",
        "    \n",
        "    display(Markdown(\"### üìä System Status\"))\n",
        "    \n",
        "    print(f\"API Key: {'‚úÖ Set' if session_state.get('api_key_submitted') else '‚ùå Not set'}\")\n",
        "    print(f\"Vector Index: {'‚úÖ ' + session_state.get('index_id', 'None') if session_state.get('index_id') else '‚ùå Not created'}\")\n",
        "    print(f\"Documents Processed: {len(session_state.get('processed_files', []))}\")\n",
        "    print(f\"Total Chunks: {session_state.get('total_chunks_stored', 0)}\")\n",
        "    print(f\"Conversation Messages: {len(session_state.get('chat_history', []))}\")\n",
        "    \n",
        "    if session_state.get('processed_files'):\n",
        "        print(f\"\\nüìÑ Processed Files:\")\n",
        "        for i, filename in enumerate(session_state['processed_files'], 1):\n",
        "            print(f\"  {i}. {filename}\")\n",
        "\n",
        "print(\"‚úÖ Main RAG system functions ready\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Interactive Usage and Examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "setup_system"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ Initializing Contextual RAG System...\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "### üöÄ Setting up Contextual RAG System"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ API key set successfully\n",
            "‚úÖ API key verified successfully!\n",
            "üîç Setting up vector index: contextual-rag-demo\n",
            "‚úÖ Created vector index: 6cc50e37-a768-46cb-9c6d-34d9a0c7d9fa\n",
            "‚úÖ Contextual RAG system ready!\n",
            "   Index ID: 6cc50e37-a768-46cb-9c6d-34d9a0c7d9fa\n",
            "   Configuration: 800 chars/chunk, 5 retrieval\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "### ‚úÖ System Ready!"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üéØ Next steps:\n",
            "1. Use `ingest_document('path/to/your/file.pdf')` to upload documents\n",
            "2. Use `query_contextual_rag('Your question here')` to ask questions\n",
            "3. Use `show_conversation_history()` to view chat history\n",
            "4. Use `clear_conversation()` to start fresh\n"
          ]
        }
      ],
      "source": [
        "# Initialize and setup the Contextual RAG system\n",
        "print(\"üöÄ Initializing Contextual RAG System...\")\n",
        "\n",
        "# Setup the system (this will verify API key and create vector index)\n",
        "setup_success = setup_contextual_rag(\"contextual-rag-demo\")\n",
        "\n",
        "if setup_success:\n",
        "    display(Markdown(\"### ‚úÖ System Ready!\"))\n",
        "    print(\"\\nüéØ Next steps:\")\n",
        "    print(\"1. Use `ingest_document('path/to/your/file.pdf')` to upload documents\")\n",
        "    print(\"2. Use `query_contextual_rag('Your question here')` to ask questions\")\n",
        "    print(\"3. Use `show_conversation_history()` to view chat history\")\n",
        "    print(\"4. Use `clear_conversation()` to start fresh\")\n",
        "else:\n",
        "    display(Markdown(\"### ‚ùå Setup Failed\"))\n",
        "    print(\"Please check your API key and try again.\")\n",
        "    print(\"Use `set_api_key('your_key_here')` if needed.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "example_usage"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "### üìÑ Ingesting Document: `Test.pdf`"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîç Extracting text...\n",
            "‚úÖ Extracted 100034 characters\n",
            "üß© Creating contextual chunks...\n",
            "‚úÖ Created 167 chunks\n",
            "üíæ Uploading to vector database...\n",
            "‚úÖ Uploaded 167 chunks successfully\n",
            "‚úÖ Document 'Test.pdf' successfully ingested!\n",
            "üìä Total documents: 1\n",
            "üìä Total chunks stored: 167\n",
            "Document successfully uploaded!\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "### üìä System Status"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîë API Key: ‚úÖ Set\n",
            "üìá Vector Index: ‚úÖ 6cc50e37-a768-46cb-9c6d-34d9a0c7d9fa\n",
            "üìö Documents Processed: 1\n",
            "üß© Total Chunks: 167\n",
            "üí¨ Conversation Messages: 0\n",
            "\n",
            "üìÑ Processed Files:\n",
            "  1. Test.pdf\n"
          ]
        }
      ],
      "source": [
        "# Example: Ingest a document (replace with your file path)\n",
        "\n",
        "# Example 1: Ingest a PDF document\n",
        "success = ingest_document('/Users/rupinajay/Downloads/Test.pdf')\n",
        "if success:\n",
        "    print(\"Document successfully uploaded!\")\n",
        "\n",
        "# Example 2: Ingest multiple documents\n",
        "# documents = ['doc1.pdf', 'doc2.txt', 'doc3.docx']\n",
        "# for doc_path in documents:\n",
        "#     if os.path.exists(doc_path):\n",
        "#         ingest_document(doc_path)\n",
        "\n",
        "# For now, let's show the current system status\n",
        "show_system_status()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "interactive_query"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "### üß† Contextual Query: What is the main topic of the document?"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
            "üí≠ Using conversation history for enhanced context...\n",
            "üîç Performing contextual retrieval...\n",
            "üîç Starting contextual search for: 'What is the main topic of the document?...'\n",
            "üìù Generated 4 search variations\n",
            "üîç Found 10 relevant chunks\n",
            "üîç Found 10 relevant chunks\n",
            "üîç Found 10 relevant chunks\n",
            "üîç Found 10 relevant chunks\n",
            "üéØ Found 17 unique results\n",
            "‚≠ê Selected top 3 most relevant chunks\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "#### üìÑ Retrieved Context:"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Recent Conversation Context:\n",
            "Human: What is the main topic of the document?\n",
            "Assistant: Based on the provided document context and conversation history, I can infer that the main topic of the document is Artificial Intelligence (AI) in the context of travel and tourism.\n",
            "\n",
            "The introduction...\n",
            "\n",
            "\n",
            "Source 1 [Score: 0.821] - Test.pdf (chunk 164, pdf):\n",
            "[PAGE 44] INTRODUCTION TO AI ¬© World Travel & Tourism Council: Introduction to AI 2024. All rights reserved. The copyright laws of the United Kingdom allow certain uses of this content without our (i.e. the copyright owner‚Äôs) permission. You are permitted to use limited extracts of this content, provided such use is fair and when such \n",
            "use is for non-commercial research, private study, review or news reporting. The following acknowledgment must also be used, whenever our content is used relying on this ‚Äúfair dealing‚Äù exception: ‚ÄúSource: World Travel and \n",
            "Tourism Council: Introduction to AI 2024.\n",
            "\n",
            "Source 2 [Score: 0.797] - Test.pdf (chunk 165, pdf...\n",
            "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "ü§ñ Generating contextual response...\n",
            "ü§ñ Generating contextual response...\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "### üí° Contextual Response:"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Based on the provided document context, recent conversation history, and the sources listed above, I can infer that the main topic of the document is Artificial Intelligence (AI) in the context of travel and tourism.\n",
            "\n",
            "The introduction to AI 2024, as mentioned in Source 1 [Score: 0.821], explicitly states that the copyright laws of the United Kingdom allow certain uses of this content without permission, provided such use falls under the \"fair dealing\" exception or complies with the Attribution, Non-Commercial 4.0 International Creative Commons Licence (Source 2 [Score: 0.797] and Source 3 [Score: 0.770]). This suggests that the document is focused on AI in travel and tourism, as it provides information about using AI in this context while adhering to copyright laws.\n",
            "\n",
            "The consistency of the sources across different chunks of the PDF (164-166) further supports this inference. The repeated mention of \"Introduction to AI 2024\" and the World Travel & Tourism Council as the source indicates that the document is centered around AI in travel and tourism.\n",
            "\n",
            "Therefore, based on the contextual information provided, I conclude that the main topic of the document is indeed Artificial Intelligence (AI) in the context of travel and tourism.\n",
            "\n",
            "üß† Retrieval Summary:\n",
            "  ‚Ä¢ Analyzed 3 relevant chunks\n",
            "  ‚Ä¢ Used conversation history: Yes\n",
            "  ‚Ä¢ Sources: Test.pdf\n",
            "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
            "\n",
            "üîß Utility Functions:\n",
            "- show_conversation_history()  # View chat history\n",
            "- clear_conversation()         # Clear chat history\n",
            "- show_system_status()         # View system status\n"
          ]
        }
      ],
      "source": [
        "# Interactive Query Interface\n",
        "# Replace this with your actual questions after ingesting documents\n",
        "\n",
        "# Example queries (uncomment after uploading documents):\n",
        "result = query_contextual_rag(\"What is the main topic of the document?\")\n",
        "# result = query_contextual_rag(\"Can you summarize the key points?\")\n",
        "# result = query_contextual_rag(\"How does this relate to what we discussed earlier?\")\n",
        "\n",
        "print(\"\\nüîß Utility Functions:\")\n",
        "print(\"- show_conversation_history()  # View chat history\")\n",
        "print(\"- clear_conversation()         # Clear chat history\")\n",
        "print(\"- show_system_status()         # View system status\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "conversation_tools"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ü§ñ Starting Interactive Contextual RAG Chat\n",
            "Type 'quit' to exit, 'clear' to clear history, 'status' for system info\n",
            "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "### üß† Contextual Query: Explain what all ai concepts used in this doc"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
            "üîç Performing contextual retrieval...\n",
            "üîç Starting contextual search for: 'Explain what all ai concepts used in this doc...'\n",
            "üìù Generated 3 search variations\n",
            "üîç Found 10 relevant chunks\n",
            "üîç Found 10 relevant chunks\n",
            "üîç Found 10 relevant chunks\n",
            "üéØ Found 25 unique results\n",
            "‚≠ê Selected top 3 most relevant chunks\n",
            "ü§ñ Generating contextual response...\n",
            "ü§ñ Generating contextual response...\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "### üí° Contextual Response:"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Based on the provided document context, recent conversation history, and sources listed above, I can infer that several AI concepts are used in this document. To explain what all AI concepts used in this doc, let's break down the relevant information from the sources:\n",
            "\n",
            "1. **Predictive AI**: According to Source 1 [Score: 0.969], Predictive AI is a type of AI system that uses current and historical data to make predictions about future events, outcomes, or behaviors. This concept is mentioned in the context of analyzing large amounts of data to identify patterns, trends, and relationships.\n",
            "2. **Expert Systems**: Source 1 [Score: 0.969] also mentions Expert Systems as a way for AI systems to draw from the expertise of many stakeholders who have fed into the 'knowledge base'. This concept is relevant in the context of providing highly tailored recommendations based on individual preferences and requirements.\n",
            "3. **AI Governance, Safety, and Regulation**: Source 3 [Score: 0.894] mentions that three further publications will explore how AI is currently being used in Travel & Tourism, and the fast-moving world of AI governance, safety, and regulation. This concept is related to the challenges and opportunities presented by AI in the tourism industry.\n",
            "4. **Introduction to AI**: Source 2 [Score: 0.944] provides an introduction to AI, explaining how it works, where it came from, and the different types of AI available today. This source serves as a foundation for understanding the various AI concepts used in this document.\n",
            "\n",
            "To connect these concepts, I can infer that the document discusses the applications and implications of AI in the tourism industry. Predictive AI is used to analyze data and make predictions about future events or behaviors. Expert Systems are employed to provide tailored recommendations based on individual preferences and requirements. The concept of AI governance, safety, and regulation highlights the challenges and opportunities presented by AI in this sector.\n",
            "\n",
            "However, I must acknowledge that there might be other AI concepts mentioned in the document that are not explicitly stated here. The sources provided only cover a portion of the document's content, and further analysis may reveal additional AI concepts used in this doc.\n",
            "\n",
            "In conclusion, based on the available information, the main AI concepts used in this document include Predictive AI, Expert Systems, AI Governance, Safety, and Regulation, and Introduction to AI. These concepts are interconnected and demonstrate how AI is being applied and explored in the tourism industry.\n",
            "\n",
            "**Limitations:** The answer provided is based on the sources listed above and may not be exhaustive. Further analysis of the document or additional context might reveal other AI concepts used in this doc.\n",
            "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
            "\n",
            "ü§ñ Assistant: Based on the provided document context, recent conversation history, and sources listed above, I can infer that several AI concepts are used in this document. To explain what all AI concepts used in this doc, let's break down the relevant information from the sources:\n",
            "\n",
            "1. **Predictive AI**: According to Source 1 [Score: 0.969], Predictive AI is a type of AI system that uses current and historical data to make predictions about future events, outcomes, or behaviors. This concept is mentioned in the context of analyzing large amounts of data to identify patterns, trends, and relationships.\n",
            "2. **Expert Systems**: Source 1 [Score: 0.969] also mentions Expert Systems as a way for AI systems to draw from the expertise of many stakeholders who have fed into the 'knowledge base'. This concept is relevant in the context of providing highly tailored recommendations based on individual preferences and requirements.\n",
            "3. **AI Governance, Safety, and Regulation**: Source 3 [Score: 0.894] mentions that three further publications will explore how AI is currently being used in Travel & Tourism, and the fast-moving world of AI governance, safety, and regulation. This concept is related to the challenges and opportunities presented by AI in the tourism industry.\n",
            "4. **Introduction to AI**: Source 2 [Score: 0.944] provides an introduction to AI, explaining how it works, where it came from, and the different types of AI available today. This source serves as a foundation for understanding the various AI concepts used in this document.\n",
            "\n",
            "To connect these concepts, I can infer that the document discusses the applications and implications of AI in the tourism industry. Predictive AI is used to analyze data and make predictions about future events or behaviors. Expert Systems are employed to provide tailored recommendations based on individual preferences and requirements. The concept of AI governance, safety, and regulation highlights the challenges and opportunities presented by AI in this sector.\n",
            "\n",
            "However, I must acknowledge that there might be other AI concepts mentioned in the document that are not explicitly stated here. The sources provided only cover a portion of the document's content, and further analysis may reveal additional AI concepts used in this doc.\n",
            "\n",
            "In conclusion, based on the available information, the main AI concepts used in this document include Predictive AI, Expert Systems, AI Governance, Safety, and Regulation, and Introduction to AI. These concepts are interconnected and demonstrate how AI is being applied and explored in the tourism industry.\n",
            "\n",
            "**Limitations:** The answer provided is based on the sources listed above and may not be exhaustive. Further analysis of the document or additional context might reveal other AI concepts used in this doc.\n",
            "üëã Goodbye!\n",
            "üí¨ Interactive chat ready! Use `interactive_chat()` to start chatting.\n",
            "üìù Or use individual functions for more control:\n",
            "   - query_contextual_rag('Your question')\n",
            "   - show_conversation_history()\n",
            "   - clear_conversation()\n"
          ]
        }
      ],
      "source": [
        "# Conversation Management Tools\n",
        "\n",
        "def interactive_chat():\n",
        "    \"\"\"Start an interactive chat session\"\"\"\n",
        "    print(\"ü§ñ Starting Interactive Contextual RAG Chat\")\n",
        "    print(\"Type 'quit' to exit, 'clear' to clear history, 'status' for system info\")\n",
        "    print(\"‚îÄ\" * 60)\n",
        "    \n",
        "    while True:\n",
        "        try:\n",
        "            user_input = input(\"\\nüßë You: \").strip()\n",
        "            \n",
        "            if user_input.lower() in ['quit', 'exit', 'bye']:\n",
        "                print(\"üëã Goodbye!\")\n",
        "                break\n",
        "            elif user_input.lower() == 'clear':\n",
        "                clear_conversation()\n",
        "                continue\n",
        "            elif user_input.lower() == 'status':\n",
        "                show_system_status()\n",
        "                continue\n",
        "            elif user_input.lower() == 'history':\n",
        "                show_conversation_history()\n",
        "                continue\n",
        "            elif not user_input:\n",
        "                print(\"Please enter a question or command.\")\n",
        "                continue\n",
        "            \n",
        "            # Process the query\n",
        "            result = query_contextual_rag(user_input, show_context=False, show_retrieval_info=False)\n",
        "            print(f\"\\nü§ñ Assistant: {result['response']}\")\n",
        "            \n",
        "        except KeyboardInterrupt:\n",
        "            print(\"\\nüëã Chat session ended.\")\n",
        "            break\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error: {e}\")\n",
        "            continue\n",
        "\n",
        "interactive_chat()\n",
        "\n",
        "print(\"üí¨ Interactive chat ready! Use `interactive_chat()` to start chatting.\")\n",
        "print(\"üìù Or use individual functions for more control:\")\n",
        "print(\"   - query_contextual_rag('Your question')\")\n",
        "print(\"   - show_conversation_history()\")\n",
        "print(\"   - clear_conversation()\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Advanced Features and Utilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "advanced_features"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Advanced features ready\n",
            "üîß Available functions:\n",
            "   - bulk_ingest_directory('/path/to/docs')\n",
            "   - search_by_metadata({'document_type': 'pdf'})\n",
            "   - export_conversation('json')\n",
            "   - get_document_summary()\n"
          ]
        }
      ],
      "source": [
        "# Advanced Features\n",
        "\n",
        "def bulk_ingest_directory(directory_path: str, supported_extensions: List[str] = None) -> List[str]:\n",
        "    \"\"\"Ingest all supported documents from a directory\"\"\"\n",
        "    if supported_extensions is None:\n",
        "        supported_extensions = ['.pdf', '.txt', '.md', '.docx']\n",
        "    \n",
        "    if not os.path.isdir(directory_path):\n",
        "        print(f\"‚ùå Directory not found: {directory_path}\")\n",
        "        return []\n",
        "    \n",
        "    display(Markdown(f\"### üìÅ Bulk Ingestion from: `{directory_path}`\"))\n",
        "    \n",
        "    ingested_files = []\n",
        "    \n",
        "    for filename in os.listdir(directory_path):\n",
        "        file_path = os.path.join(directory_path, filename)\n",
        "        file_ext = os.path.splitext(filename)[1].lower()\n",
        "        \n",
        "        if os.path.isfile(file_path) and file_ext in supported_extensions:\n",
        "            print(f\"\\nüìÑ Processing: {filename}\")\n",
        "            success = ingest_document(file_path)\n",
        "            if success:\n",
        "                ingested_files.append(filename)\n",
        "            else:\n",
        "                print(f\"‚ö†Ô∏è  Failed to ingest: {filename}\")\n",
        "    \n",
        "    print(f\"\\n‚úÖ Bulk ingestion complete: {len(ingested_files)}/{len([f for f in os.listdir(directory_path) if os.path.splitext(f)[1].lower() in supported_extensions])} files\")\n",
        "    return ingested_files\n",
        "\n",
        "def search_by_metadata(metadata_filter: Dict, top_k: int = 10) -> List[Dict]:\n",
        "    \"\"\"Search documents by metadata filters\"\"\"\n",
        "    print(f\"üîç Searching by metadata: {metadata_filter}\")\n",
        "    \n",
        "    # Use a generic query for metadata-based search\n",
        "    hits = search_vectors(\"*\", top_k=top_k, filters=metadata_filter)\n",
        "    \n",
        "    if hits:\n",
        "        print(f\"‚úÖ Found {len(hits)} documents matching filters\")\n",
        "        for i, hit in enumerate(hits, 1):\n",
        "            metadata = hit.get('metadata', {})\n",
        "            print(f\"  {i}. {metadata.get('filename', 'Unknown')} (chunk {metadata.get('chunk_index', 0)})\")\n",
        "    else:\n",
        "        print(\"‚ùå No documents found matching the filters\")\n",
        "    \n",
        "    return hits\n",
        "\n",
        "def export_conversation(format_type: str = 'json', file_path: str = None) -> str:\n",
        "    \"\"\"Export conversation history in various formats\"\"\"\n",
        "    init_session_state()\n",
        "    \n",
        "    if not session_state['chat_history']:\n",
        "        print(\"‚ùå No conversation history to export\")\n",
        "        return \"\"\n",
        "    \n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    \n",
        "    if format_type.lower() == 'json':\n",
        "        export_data = {\n",
        "            \"export_timestamp\": timestamp,\n",
        "            \"total_messages\": len(session_state['chat_history']),\n",
        "            \"conversation\": list(session_state['chat_history'])\n",
        "        }\n",
        "        export_content = json.dumps(export_data, indent=2, default=str)\n",
        "        default_filename = f\"conversation_{timestamp}.json\"\n",
        "    \n",
        "    elif format_type.lower() == 'txt':\n",
        "        lines = [f\"Conversation Export - {timestamp}\\n\", \"=\" * 50 + \"\\n\\n\"]\n",
        "        \n",
        "        for msg in session_state['chat_history']:\n",
        "            role = \"You\" if msg['role'] == 'user' else \"Assistant\"\n",
        "            timestamp = datetime.fromisoformat(msg['timestamp']).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "            lines.append(f\"[{timestamp}] {role}:\\n{msg['content']}\\n\\n\")\n",
        "        \n",
        "        export_content = \"\".join(lines)\n",
        "        default_filename = f\"conversation_{timestamp}.txt\"\n",
        "    \n",
        "    else:\n",
        "        print(f\"‚ùå Unsupported format: {format_type}\")\n",
        "        return \"\"\n",
        "    \n",
        "    output_path = file_path or default_filename\n",
        "    \n",
        "    try:\n",
        "        with open(output_path, 'w', encoding='utf-8') as f:\n",
        "            f.write(export_content)\n",
        "        print(f\"‚úÖ Conversation exported to: {output_path}\")\n",
        "        return output_path\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Export failed: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "def get_document_summary() -> Dict[str, Any]:\n",
        "    \"\"\"Get summary of ingested documents\"\"\"\n",
        "    init_session_state()\n",
        "    \n",
        "    summary = {\n",
        "        \"total_documents\": len(session_state.get('processed_files', [])),\n",
        "        \"total_chunks\": session_state.get('total_chunks_stored', 0),\n",
        "        \"document_types\": {},\n",
        "        \"files\": session_state.get('processed_files', [])\n",
        "    }\n",
        "    \n",
        "    # Count document types\n",
        "    for filename in session_state.get('processed_files', []):\n",
        "        ext = os.path.splitext(filename)[1].lower()\n",
        "        doc_type = _get_document_type(filename)\n",
        "        summary[\"document_types\"][doc_type] = summary[\"document_types\"].get(doc_type, 0) + 1\n",
        "    \n",
        "    return summary\n",
        "\n",
        "print(\"‚úÖ Advanced features ready\")\n",
        "print(\"üîß Available functions:\")\n",
        "print(\"   - bulk_ingest_directory('/path/to/docs')\")\n",
        "print(\"   - search_by_metadata({'document_type': 'pdf'})\")\n",
        "print(\"   - export_conversation('json')\")\n",
        "print(\"   - get_document_summary()\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Final Summary and Usage Guide"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "usage_guide"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "# Contextual RAG Usage Guide"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "                       CONTEXTUAL RAG SYSTEM                   \n",
            "                     Ready for Interactive Use                 \n",
            "\n",
            "\n",
            " GETTING STARTED:\n",
            "   1  setup_contextual_rag()                    # Initialize system\n",
            "   2  ingest_document('path/to/file.pdf')       # Upload documents  \n",
            "   3  query_contextual_rag('Your question')     # Ask questions\n",
            "\n",
            " DOCUMENT MANAGEMENT:\n",
            "    ingest_document('file.pdf')                # Single file\n",
            "    bulk_ingest_directory('/docs')             # Entire directory\n",
            "    get_document_summary()                     # Document overview\n",
            "    search_by_metadata({'type': 'pdf'})       # Filter by metadata\n",
            "\n",
            " CONVERSATION FEATURES:\n",
            "      query_contextual_rag('question')           # Context-aware queries\n",
            "      interactive_chat()                         # Start chat session\n",
            "      show_conversation_history()                # View chat history\n",
            "      clear_conversation()                       # Clear history\n",
            "      export_conversation('json')                # Export chat\n",
            "\n",
            " SYSTEM UTILITIES:\n",
            "    show_system_status()                       # System overview\n",
            "    verify_api_key()                          # Check API key\n",
            "    set_api_key('your_key')                   # Set new API key\n",
            "\n",
            " KEY FEATURES:\n",
            "    Contextual Understanding    - Maintains conversation flow\n",
            "    Advanced Retrieval         - Multi-stage semantic search\n",
            "    Multi-Document Support     - PDF, DOCX, TXT, Markdown\n",
            "    Memory Management          - Persistent conversation history\n",
            "    Query Enhancement          - Context-aware query expansion\n",
            "    Rich Metadata             - Document structure preservation\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "### üìä System Status"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "API Key: ‚úÖ Set\n",
            "Vector Index: ‚úÖ 6cc50e37-a768-46cb-9c6d-34d9a0c7d9fa\n",
            "Documents Processed: 1\n",
            "Total Chunks: 167\n",
            "Conversation Messages: 10\n",
            "\n",
            "üìÑ Processed Files:\n",
            "  1. Test.pdf\n",
            "\n",
            " Contextual RAG system is ready for use!\n",
            " Start by uploading documents, then ask questions to see the contextual magic! \n"
          ]
        }
      ],
      "source": [
        "# Complete Usage Guide\n",
        "display(Markdown(\"# Contextual RAG Usage Guide\"))\n",
        "\n",
        "print(\"\"\"\n",
        "                       CONTEXTUAL RAG SYSTEM                   \n",
        "                     Ready for Interactive Use                 \n",
        "\n",
        "\n",
        " GETTING STARTED:\n",
        "   1  setup_contextual_rag()                    # Initialize system\n",
        "   2  ingest_document('path/to/file.pdf')       # Upload documents  \n",
        "   3  query_contextual_rag('Your question')     # Ask questions\n",
        "\n",
        " DOCUMENT MANAGEMENT:\n",
        "    ingest_document('file.pdf')                # Single file\n",
        "    bulk_ingest_directory('/docs')             # Entire directory\n",
        "    get_document_summary()                     # Document overview\n",
        "    search_by_metadata({'type': 'pdf'})       # Filter by metadata\n",
        "\n",
        " CONVERSATION FEATURES:\n",
        "      query_contextual_rag('question')           # Context-aware queries\n",
        "      interactive_chat()                         # Start chat session\n",
        "      show_conversation_history()                # View chat history\n",
        "      clear_conversation()                       # Clear history\n",
        "      export_conversation('json')                # Export chat\n",
        "\n",
        " SYSTEM UTILITIES:\n",
        "    show_system_status()                       # System overview\n",
        "    verify_api_key()                          # Check API key\n",
        "    set_api_key('your_key')                   # Set new API key\n",
        "\n",
        " KEY FEATURES:\n",
        "    Contextual Understanding    - Maintains conversation flow\n",
        "    Advanced Retrieval         - Multi-stage semantic search\n",
        "    Multi-Document Support     - PDF, DOCX, TXT, Markdown\n",
        "    Memory Management          - Persistent conversation history\n",
        "    Query Enhancement          - Context-aware query expansion\n",
        "    Rich Metadata             - Document structure preservation\n",
        "\n",
        "\"\"\")\n",
        "\n",
        "# Show current system status\n",
        "show_system_status()\n",
        "\n",
        "print(\"\\n Contextual RAG system is ready for use!\")\n",
        "print(\" Start by uploading documents, then ask questions to see the contextual magic! \")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
